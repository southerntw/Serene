{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Chatbot Logic"
      ],
      "metadata": {
        "id": "OhsuXIaseaMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "CYzeLORKeO28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqq transformers datasets"
      ],
      "metadata": {
        "id": "xzpRIZBWeQAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import random\n",
        "import pandas as pd\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from IPython.display import Markdown, display\n",
        "from datasets import Dataset"
      ],
      "metadata": {
        "id": "bR9j36zueV4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    os.chdir(\"/content/drive/MyDrive/skripsi\")\n",
        "    print(\"Directory changed\")\n",
        "except OSError:\n",
        "    print(\"Error: Can't change the Current Working Directory\")"
      ],
      "metadata": {
        "id": "LGFhiakrelzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "start_message = \"==== Hello! I am Alex and I am your virtual friend. If you need a listening ear, I'm always here. To end the chat, input 'exit' in the chatbox. ====\"\n",
        "\n",
        "prevention_messages = [\"Are you okay? How long have you been feeling this way?\",\n",
        "                       \"That sounds so painful, and I appreciate you sharing that with me. How can I help?\",\n",
        "                       \"I know things seem bleak now, but it can be hard to see possible solutions when you feel so overwhelmed.\",\n",
        "                       \"I'm concerned about you because I care, and I want to offer support however I can. You can talk to me.\",\n",
        "                       \"I'm always here if you feel like talking.\",\n",
        "                       \"I'm always here to listen, but do you think a therapist could help a little more?\",\n",
        "                       \"Have you thought about talking to a therapist?\",\n",
        "                       \"You can withstand any storm and when you are too tired to stand, I will hold you up. You are never alone.\",\n",
        "                       \"You know I’m always here for you.\",\n",
        "                       \"You’re allowed to have bad days, but remember tomorrow is a brand new day.\",\n",
        "                       \"You’ve got a place here on Earth for a reason.\",\n",
        "                       \"It's okay to have such thoughts but if they become overwhelming, don't keep it to yourself. I am here for you.\",\n",
        "                       \"Everything is a season, and right now you’re in winter. It’s dark and cold and you can’t find shelter, but one day it’ll be summer, and you’ll look back and be grateful you stuck it out through winter.\",\n",
        "                       \"I know you are going through a lot and you’re scared, but you will never lose me.\",\n",
        "                       \"I know it feels like a lot right now. It’s OK to feel that way.\",\n",
        "                       \"Is there anything I can do to make this day go easier for you?\"]\n",
        "\n",
        "helpline_message = \"In times of severe distress where you need to speak with someone immediately, these are suicide hotline services available for you. You will be speaking with volunteers or professionals who are trained to deal with suicide crisis. Samaritans of Singapore (SOS; 24 hours): 1800 221 4444 Mental Health Helpline (24 hours): 6389 2222 Singapore Association for Mental Health (SAMH) Helpline: 1800 283 7019\""
      ],
      "metadata": {
        "id": "zua72tEKeo62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions"
      ],
      "metadata": {
        "id": "nZTZMxsOeyGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def printmd(string):\n",
        "    display(Markdown(string))"
      ],
      "metadata": {
        "id": "9xIriDmQeuL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Untuk load tokenizer dan model generate response\n",
        "def load_tokenizer_and_model(model=\"microsoft/DialoGPT-large\"):\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "  model = AutoModelForCausalLM.from_pretrained(model)\n",
        "\n",
        "  return tokenizer, model"
      ],
      "metadata": {
        "id": "SSMsnOKpe0FP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Untuk load tokenizer dan model sentiment analysis (detect suicidal intention)\n",
        "def load_suicide_tokenizer_and_model(tokenizer=\"bert-base-uncased\", model='./Models/bert/'):\n",
        "  suicide_tokenizer = AutoTokenizer.from_pretrained(tokenizer)\n",
        "  suicide_model = AutoModelForSequenceClassification.from_pretrained('./Models/bert/')\n",
        "\n",
        "  return suicide_tokenizer, suicide_model"
      ],
      "metadata": {
        "id": "YIuVzeH9e3IB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Detecting Suicidal Intention\n",
        "def check_intent(text):\n",
        "  global suicide_tokenizer, suicide_model\n",
        "  tokenised_text = suicide_tokenizer.encode_plus(text, return_tensors=\"pt\")\n",
        "  logits = suicide_model(**tokenised_text)[0]\n",
        "  prediction = round(torch.softmax(logits, dim=1).tolist()[0][1])\n",
        "  return prediction"
      ],
      "metadata": {
        "id": "hyFZfZe0fBaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate Response\n",
        "# Requires Editing\n",
        "def generate_response(tokenizer, model, chat_round, chat_history_ids):\n",
        "  user_input = input(\">> You: \")\n",
        "\n",
        "  if user_input == \"exit\":\n",
        "    raise Exception(\"End of Conversation\")\n",
        "\n",
        "  # Encode user input and End-of-String (EOS) token\n",
        "  new_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt')\n",
        "\n",
        "  # Append tokens to chat history\n",
        "  bot_input_ids = torch.cat([chat_history_ids, new_input_ids], dim = -1) if chat_round > 0 else new_input_ids\n",
        "\n",
        "  # Generate response given maximum chat length history of 1250 tokens\n",
        "  chat_history_ids = model.generate(bot_input_ids, max_length=1250, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "  # Print response based on intent\n",
        "  if check_intent(user_input):\n",
        "    printmd(\"*Alex:* {}\".format(random.choice(prevention_messages)))\n",
        "    printmd(\"{}\".format(helpline_message))\n",
        "  else:\n",
        "    printmd(\"*Alex:* {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n",
        "\n",
        "  # Return the chat history ids\n",
        "  return chat_history_ids"
      ],
      "metadata": {
        "id": "4MLOuQTffIPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize chatbot tokenizer and model\n",
        "tokenizer, model = load_tokenizer_and_model()\n",
        "\n",
        "# Initialize chatbot history variable\n",
        "chat_history_ids = None\n",
        "\n",
        "# Initialise suicide detection tokenizer and model\n",
        "suicide_tokenizer, suicide_model = load_suicide_tokenizer_and_model()"
      ],
      "metadata": {
        "id": "cj4G1bv5fOzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Edit sebagaimana mungkin...\n",
        "def start_chatbot(n=1000):\n",
        "  global tokenizer, model, chat_history_ids\n",
        "\n",
        "  print(start_message)\n",
        "\n",
        "  try:\n",
        "    for chat_round in range(n):\n",
        "      chat_history_ids = generate_response(tokenizer, model, chat_round, chat_history_ids)\n",
        "  except Exception as e:\n",
        "    printmd(\"*Alex:* See ya\")"
      ],
      "metadata": {
        "id": "V8QlWAIHfV5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Serverside (FastAPI)\n",
        "\n"
      ],
      "metadata": {
        "id": "LLSN9scxeJ_g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxNaSSFV3v5D",
        "outputId": "8d90e737-8a45-4f0e-ba25-4b9737a529d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastapi\n",
            "  Downloading fastapi-0.104.0-py3-none-any.whl (92 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/92.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m92.2/92.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (1.5.8)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.0.0.tar.gz (718 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m718.7/718.7 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi) (3.7.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (1.10.13)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions>=4.8.0 (from fastapi)\n",
            "  Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi) (1.1.3)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-7.0.0-py3-none-any.whl size=21129 sha256=ed4e78fe1a7b4297c1435cc93d979ef22ca3cf49a2afade071f60d7c3eeca60c\n",
            "  Stored in directory: /root/.cache/pip/wheels/60/29/7b/f64332aa7e5e88fbd56d4002185ae22dcdc83b35b3d1c2cbf5\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: typing-extensions, pyngrok, h11, uvicorn, starlette, fastapi\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "tensorflow 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed fastapi-0.104.0 h11-0.14.0 pyngrok-7.0.0 starlette-0.27.0 typing-extensions-4.8.0 uvicorn-0.23.2\n"
          ]
        }
      ],
      "source": [
        "!pip install fastapi nest-asyncio pyngrok uvicorn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\"Are you okay? How long have you been feeling this way?\",\n",
        "                       \"That sounds so painful, and I appreciate you sharing that with me. How can I help?\",\n",
        "                       \"I know things seem bleak now, but it can be hard to see possible solutions when you feel so overwhelmed.\",\n",
        "                       \"I'm concerned about you because I care, and I want to offer support however I can. You can talk to me.\",\n",
        "                       \"I'm always here if you feel like talking.\",\n",
        "                       \"I'm always here to listen, but do you think a therapist could help a little more?\",\n",
        "                       \"Have you thought about talking to a therapist?\",\n",
        "                       \"You can withstand any storm and when you are too tired to stand, I will hold you up. You are never alone.\",\n",
        "                       \"You know I’m always here for you.\",\n",
        "                       \"You’re allowed to have bad days, but remember tomorrow is a brand new day.\",\n",
        "                       \"You’ve got a place here on Earth for a reason.\",\n",
        "                       \"It's okay to have such thoughts but if they become overwhelming, don't keep it to yourself. I am here for you.\",\n",
        "                       \"Everything is a season, and right now you’re in winter. It’s dark and cold and you can’t find shelter, but one day it’ll be summer, and you’ll look back and be grateful you stuck it out through winter.\",\n",
        "                       \"I know you are going through a lot and you’re scared, but you will never lose me.\",\n",
        "                       \"I know it feels like a lot right now. It’s OK to feel that way.\",\n",
        "                       \"Is there anything I can do to make this day go easier for you?\"]\n",
        "\n"
      ],
      "metadata": {
        "id": "YXsnfkZVI-NV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDMH0XBR3425"
      },
      "outputs": [],
      "source": [
        "from fastapi import FastAPI\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "import random\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=['*'],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=['*'],\n",
        "    allow_headers=['*'],\n",
        ")\n",
        "\n",
        "@app.get('/chat/{chat_id}')\n",
        "async def get_chat(chat_id):\n",
        "  return {\"item_id\": chat_id}\n",
        "\n",
        "@app.get('/chat/send/{send}')\n",
        "async def send_chat(send):\n",
        "  return {\"response\": random.choice(messages)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxdJPiPv3-bc",
        "outputId": "b5159f14-2394-4695-cb12-64919a0f1479"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2023-10-20T04:35:08+0000 lvl=warn msg=\"ngrok config file found at legacy location, move to XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\n",
            "INFO:     Started server process [534]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Public URL: https://9a07-34-74-209-146.ngrok.io\n",
            "INFO:     103.195.142.239:0 - \"GET / HTTP/1.1\" 404 Not Found\n",
            "INFO:     103.195.142.239:0 - \"GET /chat/send/hello HTTP/1.1\" 200 OK\n",
            "INFO:     103.195.142.239:0 - \"GET /chat/send/yeah HTTP/1.1\" 200 OK\n",
            "INFO:     103.195.142.239:0 - \"GET /chat/send/test HTTP/1.1\" 200 OK\n",
            "INFO:     103.195.142.239:0 - \"GET /chat/send/hello HTTP/1.1\" 200 OK\n",
            "INFO:     103.195.142.239:0 - \"GET /chat/send/something HTTP/1.1\" 200 OK\n",
            "INFO:     103.195.142.239:0 - \"GET /chat/send/test HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [534]\n"
          ]
        }
      ],
      "source": [
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "\n",
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "print('Public URL:', ngrok_tunnel.public_url)\n",
        "nest_asyncio.apply()\n",
        "uvicorn.run(app, port=8000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "fgme3aCcBqTO",
        "outputId": "d3d53793-05fd-4a6c-a7c0-7fee8e09788b"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-d7b983f2448e>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfastapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemplating\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mJinja2Templates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0muvicorn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mchatterbot\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChatBot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchatterbot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChatterBotCorpusTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfastapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmiddleware\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCORSMiddleware\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'chatterbot'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from fastapi import FastAPI, Request\n",
        "from fastapi.responses import HTMLResponse\n",
        "from fastapi.templating import Jinja2Templates\n",
        "import uvicorn\n",
        "from chatterbot import ChatBot\n",
        "from chatterbot.trainers import ChatterBotCorpusTrainer\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=['*'],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=['*'],\n",
        "    allow_headers=['*'],\n",
        ")\n",
        "\n",
        "templates = Jinja2Templates(directory=\"templates\")\n",
        "\n",
        "english_bot = ChatBot(\"Chatterbot\")\n",
        "trainer = ChatterBotCorpusTrainer(english_bot)\n",
        "trainer.train(\"chatterbot.corpus.english\")\n",
        "\n",
        "@app.get(\"/\", response_class=HTMLResponse)\n",
        "def home(request: Request):\n",
        "    return templates.TemplateResponse(\"index.html\", {\"request\": request})\n",
        "\n",
        "@app.get(\"/getChatBotResponse\")\n",
        "def get_bot_response(msg: str):\n",
        "    return str(english_bot.get_response(msg))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}