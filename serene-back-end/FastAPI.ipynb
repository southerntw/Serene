{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Chatbot Logic"
      ],
      "metadata": {
        "id": "OhsuXIaseaMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "CYzeLORKeO28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a063afd9-a521-4deb-e11a-ab22df3a44bd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqq transformers datasets"
      ],
      "metadata": {
        "id": "xzpRIZBWeQAx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import random\n",
        "import pandas as pd\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from IPython.display import Markdown, display\n",
        "from datasets import Dataset"
      ],
      "metadata": {
        "id": "bR9j36zueV4i"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    os.chdir(\"/content/drive/MyDrive/skripsi\")\n",
        "    print(\"Directory changed\")\n",
        "except OSError:\n",
        "    print(\"Error: Can't change the Current Working Directory\")"
      ],
      "metadata": {
        "id": "LGFhiakrelzP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aac98158-1e26-41b1-c26f-1e2f0746e188"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory changed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "start_message = \"==== Hello! I am Alex and I am your virtual friend. If you need a listening ear, I'm always here. To end the chat, input 'exit' in the chatbox. ====\"\n",
        "\n",
        "prevention_messages = [\"Are you okay? How long have you been feeling this way?\",\n",
        "                       \"That sounds so painful, and I appreciate you sharing that with me. How can I help?\",\n",
        "                       \"I know things seem bleak now, but it can be hard to see possible solutions when you feel so overwhelmed.\",\n",
        "                       \"I'm concerned about you because I care, and I want to offer support however I can. You can talk to me.\",\n",
        "                       \"I'm always here if you feel like talking.\",\n",
        "                       \"I'm always here to listen, but do you think a therapist could help a little more?\",\n",
        "                       \"Have you thought about talking to a therapist?\",\n",
        "                       \"You can withstand any storm and when you are too tired to stand, I will hold you up. You are never alone.\",\n",
        "                       \"You know I’m always here for you.\",\n",
        "                       \"You’re allowed to have bad days, but remember tomorrow is a brand new day.\",\n",
        "                       \"You’ve got a place here on Earth for a reason.\",\n",
        "                       \"It's okay to have such thoughts but if they become overwhelming, don't keep it to yourself. I am here for you.\",\n",
        "                       \"Everything is a season, and right now you’re in winter. It’s dark and cold and you can’t find shelter, but one day it’ll be summer, and you’ll look back and be grateful you stuck it out through winter.\",\n",
        "                       \"I know you are going through a lot and you’re scared, but you will never lose me.\",\n",
        "                       \"I know it feels like a lot right now. It’s OK to feel that way.\",\n",
        "                       \"Is there anything I can do to make this day go easier for you?\"]\n",
        "\n",
        "helpline_message = \"In times of severe distress where you need to speak with someone immediately, these are suicide hotline services available for you. You will be speaking with volunteers or professionals who are trained to deal with suicide crisis. Samaritans of Singapore (SOS; 24 hours): 1800 221 4444 Mental Health Helpline (24 hours): 6389 2222 Singapore Association for Mental Health (SAMH) Helpline: 1800 283 7019\""
      ],
      "metadata": {
        "id": "zua72tEKeo62"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions"
      ],
      "metadata": {
        "id": "nZTZMxsOeyGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def printmd(string):\n",
        "    display(Markdown(string))"
      ],
      "metadata": {
        "id": "9xIriDmQeuL5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Untuk load tokenizer dan model generate response\n",
        "def load_tokenizer_and_model(model=\"microsoft/DialoGPT-large\"):\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "  model = AutoModelForCausalLM.from_pretrained(model)\n",
        "\n",
        "  return tokenizer, model"
      ],
      "metadata": {
        "id": "SSMsnOKpe0FP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Untuk load tokenizer dan model sentiment analysis (detect suicidal intention)\n",
        "def load_suicide_tokenizer_and_model(tokenizer=\"bert-base-uncased\", model='./Models/bert/'):\n",
        "  suicide_tokenizer = AutoTokenizer.from_pretrained(tokenizer)\n",
        "  suicide_model = AutoModelForSequenceClassification.from_pretrained('./Models/bert/')\n",
        "\n",
        "  return suicide_tokenizer, suicide_model"
      ],
      "metadata": {
        "id": "YIuVzeH9e3IB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Detecting Suicidal Intention\n",
        "def check_intent(text):\n",
        "  global suicide_tokenizer, suicide_model\n",
        "  tokenised_text = suicide_tokenizer.encode_plus(text, return_tensors=\"pt\")\n",
        "  logits = suicide_model(**tokenised_text)[0]\n",
        "  prediction = round(torch.softmax(logits, dim=1).tolist()[0][1])\n",
        "  return prediction"
      ],
      "metadata": {
        "id": "hyFZfZe0fBaV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate Response\n",
        "# Requires Editing\n",
        "def generate_response(tokenizer, model, chat_round, chat_history_ids):\n",
        "  user_input = input(\">> You: \")\n",
        "\n",
        "  if user_input == \"exit\":\n",
        "    raise Exception(\"End of Conversation\")\n",
        "\n",
        "  # Encode user input and End-of-String (EOS) token\n",
        "  new_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt')\n",
        "\n",
        "  # Append tokens to chat history\n",
        "  bot_input_ids = torch.cat([chat_history_ids, new_input_ids], dim = -1) if chat_round > 0 else new_input_ids\n",
        "\n",
        "  # Generate response given maximum chat length history of 1250 tokens\n",
        "  chat_history_ids = model.generate(bot_input_ids, max_length=1250, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "  # Print response based on intent\n",
        "  if check_intent(user_input):\n",
        "    printmd(\"*Alex:* {}\".format(random.choice(prevention_messages)))\n",
        "    printmd(\"{}\".format(helpline_message))\n",
        "  else:\n",
        "    printmd(\"*Alex:* {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n",
        "\n",
        "  # Return the chat history ids\n",
        "  return chat_history_ids"
      ],
      "metadata": {
        "id": "4MLOuQTffIPm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize chatbot tokenizer and model\n",
        "tokenizer, model = load_tokenizer_and_model()\n",
        "\n",
        "# Initialize chatbot history variable\n",
        "chat_history_ids = None\n",
        "\n",
        "# Initialise suicide detection tokenizer and model\n",
        "suicide_tokenizer, suicide_model = load_suicide_tokenizer_and_model()"
      ],
      "metadata": {
        "id": "cj4G1bv5fOzE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Edit sebagaimana mungkin...\n",
        "def start_chatbot(n=1000):\n",
        "  global tokenizer, model, chat_history_ids\n",
        "\n",
        "  print(start_message)\n",
        "\n",
        "  try:\n",
        "    for chat_round in range(n):\n",
        "      chat_history_ids = generate_response(tokenizer, model, chat_round, chat_history_ids)\n",
        "  except Exception as e:\n",
        "    printmd(\"*Alex:* See ya\")"
      ],
      "metadata": {
        "id": "V8QlWAIHfV5m"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Serverside (FastAPI)\n",
        "\n"
      ],
      "metadata": {
        "id": "LLSN9scxeJ_g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxNaSSFV3v5D",
        "outputId": "71e43d11-3fec-4862-d068-190e3ddc5e8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastapi==0.103.2 in /usr/local/lib/python3.10/dist-packages (0.103.2)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (1.5.8)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.0.3)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.10/dist-packages (0.24.0.post1)\n",
            "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi==0.103.2) (3.7.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi==0.103.2) (1.10.13)\n",
            "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from fastapi==0.103.2) (0.27.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from fastapi==0.103.2) (4.5.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi==0.103.2) (3.6)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi==0.103.2) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi==0.103.2) (1.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install fastapi==0.103.2 nest-asyncio pyngrok uvicorn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\"Are you okay? How long have you been feeling this way?\",\n",
        "                       \"That sounds so painful, and I appreciate you sharing that with me. How can I help?\",\n",
        "                       \"I know things seem bleak now, but it can be hard to see possible solutions when you feel so overwhelmed.\",\n",
        "                       \"I'm concerned about you because I care, and I want to offer support however I can. You can talk to me.\",\n",
        "                       \"I'm always here if you feel like talking.\",\n",
        "                       \"I'm always here to listen, but do you think a therapist could help a little more?\",\n",
        "                       \"Have you thought about talking to a therapist?\",\n",
        "                       \"You can withstand any storm and when you are too tired to stand, I will hold you up. You are never alone.\",\n",
        "                       \"You know I’m always here for you.\",\n",
        "                       \"You’re allowed to have bad days, but remember tomorrow is a brand new day.\",\n",
        "                       \"You’ve got a place here on Earth for a reason.\",\n",
        "                       \"It's okay to have such thoughts but if they become overwhelming, don't keep it to yourself. I am here for you.\",\n",
        "                       \"Everything is a season, and right now you’re in winter. It’s dark and cold and you can’t find shelter, but one day it’ll be summer, and you’ll look back and be grateful you stuck it out through winter.\",\n",
        "                       \"I know you are going through a lot and you’re scared, but you will never lose me.\",\n",
        "                       \"I know it feels like a lot right now. It’s OK to feel that way.\",\n",
        "                       \"Is there anything I can do to make this day go easier for you?\"]\n",
        "\n"
      ],
      "metadata": {
        "id": "YXsnfkZVI-NV"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U google-generativeai"
      ],
      "metadata": {
        "id": "RyE5tQuEgRJS"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "import textwrap\n",
        "\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "model = genai.GenerativeModel('gemini-pro')\n",
        "chat = model.start_chat(history=[])\n",
        "chat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veugYIz0gRwz",
        "outputId": "088d1075-3e6a-427a-a0e6-bf7ec96cc103"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<google.generativeai.generative_models.ChatSession at 0x79d7bd0266e0>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken $NGROK_API_KEY"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tS0Hrv069pDC",
        "outputId": "26ba9dcb-293d-4691-fb51-077ef7078aa5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "BDMH0XBR3425",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "997b297b-e0e3-4965-e89e-022d8af478f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [55376]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: https://b810-35-223-115-199.ngrok-free.app\n",
            "INFO:     2001:448a:7030:a41c:e7e3:d6f4:9f6f:6360:0 - \"GET / HTTP/1.1\" 404 Not Found\n",
            "INFO:     2001:448a:7030:a41c:e7e3:d6f4:9f6f:6360:0 - \"GET /docs HTTP/1.1\" 200 OK\n",
            "INFO:     2001:448a:7030:a41c:e7e3:d6f4:9f6f:6360:0 - \"GET /openapi.json HTTP/1.1\" 200 OK\n",
            "INFO:     2001:448a:7030:a41c:e7e3:d6f4:9f6f:6360:0 - \"POST /chat/send/ HTTP/1.1\" 200 OK\n",
            "INFO:     2001:448a:7030:a41c:e7e3:d6f4:9f6f:6360:0 - \"POST /chat/send/ HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [55376]\n"
          ]
        }
      ],
      "source": [
        "from fastapi import FastAPI, Request, Body\n",
        "from typing import Optional\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "import random\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class Message(BaseModel):\n",
        "  userMessage: str\n",
        "  userId: int\n",
        "\n",
        "class Response():\n",
        "  message: str = \"\"\n",
        "  sentiment: bool = 0\n",
        "\n",
        "request_object = { \"userMessage\": \"this is the message\", \"userId\": 69 }\n",
        "test = Body(..., example=request_object, description=\"The message content\", embed=True)\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=['*'],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=['*'],\n",
        "    allow_headers=['*'],\n",
        ")\n",
        "\n",
        "@app.post('/chat/send/')\n",
        "async def send_chat(\n",
        "    message: Message = test\n",
        "):\n",
        "    response_object = Response()\n",
        "    bot_message = chat.send_message(message.userMessage)\n",
        "    check_result = check_intent(message.userMessage)\n",
        "\n",
        "    response_object.sentiment = check_result\n",
        "    if (check_result):\n",
        "      response_object.message = random.choice(messages)\n",
        "    else:\n",
        "      response_object.message = bot_message.text\n",
        "\n",
        "    response = {\n",
        "        \"message\": response_object.message,\n",
        "        \"sentiment\": response_object.sentiment\n",
        "    }\n",
        "\n",
        "    return response\n",
        "\n",
        "\n",
        "# Ngrok\n",
        "from google.colab import userdata\n",
        "NGROK_API_KEY = userdata.get('NGROK_API_KEY')\n",
        "\n",
        "ngrok.Client(NGROK_API_KEY)\n",
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "print('Public URL:', ngrok_tunnel.public_url)\n",
        "nest_asyncio.apply()\n",
        "uvicorn.run(app, port=8000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxdJPiPv3-bc",
        "outputId": "17d37cda-7bd4-4981-f412-cfd2dc0c1ae8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: https://b2c1-35-223-115-199.ngrok-free.app\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [216]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     2001:448a:7030:a41c:e7e3:d6f4:9f6f:6360:0 - \"GET /docs HTTP/1.1\" 200 OK\n",
            "INFO:     2001:448a:7030:a41c:e7e3:d6f4:9f6f:6360:0 - \"GET /openapi.json HTTP/1.1\" 200 OK\n",
            "string\n",
            "INFO:     2001:448a:7030:a41c:e7e3:d6f4:9f6f:6360:0 - \"POST /chat/send/ HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [216]\n"
          ]
        }
      ],
      "source": [
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "\n",
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "print('Public URL:', ngrok_tunnel.public_url)\n",
        "nest_asyncio.apply()\n",
        "uvicorn.run(app, port=8000)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}